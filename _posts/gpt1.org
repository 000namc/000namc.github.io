#+TITLE: (GPT-1) Improving Language Understanding by Generative Pre-Training
#+LAYOUT: post
#+jekyll_tags: nlp
#+jekyll_categories: AI-Research
#+DATE: 2024-11-07

- 상위포스트 : nlp 분야 논문리뷰 및 구현
- 논문 : https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- 구현 : https://github.com/000namc/paper-implementations/tree/main/vision

<hr>

https://000namc.xyz/nginx/blog/gpt1/figure1.jpeg

*** Abstract
GPT-1은 라벨링 되지 않은 대규모 corpus에서 언어 모델을 generative pre-training 하였다고 합니다. 이후 각 task에 맞추어 미세 조정함으로써 큰 성능 향상을 달성 할 수 있었다고 설명합니다. 
*** Introduction
 대부분의 딥러닝 모델은 상당한 양의 labeled 데이터가 필요합니다. 이는 annotate되지 않은 task들에서 크게 어려움으로 작용하는데, 이러한 상황에서 unlabeled 데이터에서 언어 정보를 학습할 수 있는 모델은 중요한 대안이 될것이라 설명합니다. 이 논문에서는 un-supervised 사전학습과 각 task에 맞는 supervised fine-tuning을 다룹니다. 모델 아키텍처로는 다양한 작업에서 우수한 성능을 보이는 Transformer를 사용하겠다고 합니다. 
*** Related Work
**** Semi-supervised learning for NLP
GPT-1은 크게보면 semi-supervised learning 이라 할 수 있겠습니다. 이 방식은 초기에 unlabeled 데이터에서 word 수준, 또는 phrase 수준의 통계를 계산하고 이를 지도학습에 활용하는 식으로 구성되었었는데, unlabeled 데이터를 이용하여 word embedding을 학습하여 활용하는 형태로 발전되어 왔습니다.  
**** Unsupervised pre-training
unsupervised pre-training 은 semi-supervised learning의 일부입니다. vision 영역에서 사전학습이 신경망의 성능을 크게 향상 시킨다는 것을 확인 하였고, nlp 분야에서도 이러한 연구가 이어지고 있었습니다. LSTM 모델 등을 이용하여 사전학습을 하고 task에 맞게 fine-tuning을 하는 연구들이 이뤄지고 있었지만, 보다 긴 범위의 언어구조를 파악할 수 있는 Transformer의 구조를 사용하여 더 큰 개선을 이루어 냈다고 설명하고 있습니다. 
**** Auxiliary training objectives
auxiliary training이란 특정 task의 학습을 하는데 있어서 보조적인 학습 목표를 제시하는 연구를 지칭합니다. 이것도 역시 semi-supervised learning의 일부인데, 이 연구에서도 비지도 사전학습 만으로도 충분했지만, 보조적인 학습목표를 활용하여 성능을 더 끌어 올렸다고 설명하고 있습니다. 
*** Framework
**** Unsupervised pre-training
 이 연구에서 사용한 모델은 multi-layer Transformer decoder라고 하고, 이 모델을 학습하기 위해 likelihood $L_1$ 을 maximize 하도록 설계하였다고 합니다:

$$
L_1(U) = \sum_i \text{log}P(u_i | u_{i-k}, \cdots , u_{i-1})
 $$

 여기서 $k$ 는 context window의 사이즈이고, $u_i$ 들은 $i$ 번째 토큰을 지칭합니다.  이러한 세팅하에 사전학습된 모델은, k개 이하의 context 토큰을 입력으로 넣으면 다음으로 나올 가능성이 높은 토큰을 생성하는 모델이 되겠습니다. 
**** Supervised fine-tuning
fine tuning을 하기 위해서, task specific한 loss, $L_2$ , 를 기준으로만 학습할 수 도 있지만, 사전 학습에 사용되었던 loss를 더한 형태
$\lambda L_1 + L_2$ 로 사용하면 좀 더 일반화가 잘 되는 형태로 모델이 학습 된다고 합니다. 
**** Task-specific input transformations
https://000namc.xyz/nginx/blog/gpt1/figure2.jpeg
일반적인 text classification의 경우 외 특별한 task들의 경우, 모델의 input을 그림과 같이 구성하는것이 유리하다고 합니다. 

*** Experiments
*** Analysis
*** Conclusion
이 논문에서는 생성적 사전학습한 언어모델을 소개했습니다. 이를 이용하여 다양한 task로의 fine-tuning이 가능하다는것을 확인하였고, 많이 경우에 state-of-the-art 인 성능을 얻어 낼 수 었어냈습니다. 앞으로 언어모델의 비지도 학습에 대한 연구가 계속 이어지길 기대한다고 언급하며 마무리 합니다. 
*** Reference
- Radford, Alec. "Improving language understanding by generative pre-training." (2018).
