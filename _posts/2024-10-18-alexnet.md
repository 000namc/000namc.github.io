---
title: "(AlexNet) ImageNet Classification with Deep Convolutional Neural Networks 리뷰"
date: 2024-10-18
layout: post
categories: 
- AI-Research
tags: 
- vision
---

-   상위포스트 : vision 분야 논문리뷰 및 구현
-   논문 : <https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf>
-   구현 : <https://github.com/000namc/paper-implementations/tree/main/vision>


<a id="orgf070ee0"></a>

## (AlexNet) ImageNet Classification with Deep Convolutional Neural Networks

![img](http://39.115.110.177:40000/nginx/blog/alexnet/figure1.jpeg)


<a id="orgd8e9e00"></a>

### Abstract

AlexNet은 이미지 분류 작업에서 획기적인 발전을 이뤄낸 convolutional neural network(CNN)입니다. 이 모델은 ImageNet LSVRC-2010 데이터셋을 활용해 학습되었으며, top-1 오류율 37.5%, top-5 오류율 17%로 당시 최고 성능을 기록했다고 합니다. AlexNet은 약 6천만 개의 파라미터와 65만 개의 뉴런으로 구성된 깊은 컨볼루션 네트워크로, 과적합 문제를 완화하기 위해 드롭아웃(dropout) 기법을 도입했다고 설명합니다. 이 모델은 ILSVRC-2012 대회에서 top-5 오류율 15.3%를 기록했으며, 이는 2등 모델의 26.2%와 비교할 때 상당한 차이를 보이며 딥러닝의 성능을 입증하였습니다.


<a id="orge718aa9"></a>

### Introduction

천만개가 넘는 이미지로부터 천개의 카테고리를 분류하는 작업을 학습하기 위해선, 학습능력이 큰 모델이 필요하고 CNN(convolutional neural network)가 필요하다고 주장 합니다. CNN은 이미지의 통계적 성질을 보다 잘 반영하여 기존 신경망보다 더 적은 파라미터로 더 효율적인 학습이 가능하다고 합니다. 더욱이 최신 GPU의 발전으로 2D convolution의 최적화된 처리가 가능하게 되었고 그런 이유로 고해상도의 이미지 데이터를 처리하는데에 어려움이 해소되었다고 설명합니다.
이 논문에서 제시한 기여는 다음과 같습니다. ILSVRC-2010과 ILSVRC-2012에서 사용된 ImageNet 데이터셋의 일부를 사용해 당대 가장 큰 CNN 중 하나를 학습시켜 최고 성능을 기록했으며, GPU에 최적화된 2D 컨볼루션 구현을 통해 학습 시간을 단축시켰습니다.
과적합을 방지하기위해 드롭아웃 기법이 적용되었고, 최종 네트워크는 5개의 convolution layer와 3개의 fully connected layer로 구성하였고 이러한 구성이 성능향상에 중요한 역할을 했다고 설명하고 있습니다. 


<a id="org3867fe7"></a>

### The Dataset

2010년을 시작으로 ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 라는 대회가 개최되었다고 합니다. 이 대회에 주어진 데이터셋 이름이 ImageNet이고, 이는 1000개의 카테고리에 대해 각 대략 1000개씩의 이미지로 이루어진 데이터셋이라고 설명합니다. 전체적으로는 120만개의 학습이미지, 5만개의 검증 이미지, 15만개의 테스트 이미지로 구성되어 있다고 합니다. 이 데이터셋의 이미지의 해상도는 가변 해상도로 주어진다고 합니다, 특별히 이 논문의 실험하는 과정에서는 256by256 해상도로 다운샘플링하여 사용했다고 설명합니다. 


<a id="org02d74ef"></a>

### The Architecture

1.  ReLU Nonlinearity

    2010년에 제안된 ReLUs activation 을 사용하였을때 tangent hyperbolic activation을 사용한것보다 학습속도가 월등히 좋았더라고 설명합니다. 빠른 학습은 또한 큰 모델과, 큰 데이터셋에 있어서 좋은 영향을 준다 라고 설명하고 있습니다. 다만 이렇게 제안된 ReLUs activation이 overfitting의 문제를 해결하는 부분과는 크게 관련이 있지 않다라는 점을 한번 짚고 넘어갑니다. 

2.  Training on Multiple GPUs

    single GPU로는 모델을 다 담을 수 없어 두개의 GPU를 활용했다고 설명합니다.

3.  Local Response Normalization

    relu activation은 특별히 정규화가 필요로하지 않지만, 정규화를 해주는 자체가 학습에 도움이 된다고 설명하고 있습니다. brightness normalization 라고 부르는 정규화를 적용하는것으로 top-1, top-5 error rate를 각각 1.4%, 1.2% 낮출 수 있었다 라고 설명합니다.

4.  Overlapping Pooling

    Pooling layer는 같은 kernel map내의 neurons들의 값을 집계합니다. 보통은 pooling layer를 겹치지 않도록 설계해 왔지만, 겹치게 설계하는것이 overfitting을 방지하는데에 도움이 되는것을 확인하였다고 합니다. 

5.  Overall Architecture


<a id="org2c3e280"></a>

### Reducing Overfitting

1.  Data Augmentation

    이미지 데이터에서 과적합을 줄이는 가장 쉬운 방법은 label-preserving transformations 라고 설명합니다. 이 논문에서 적용한 데이터 증강은 두가지가 있다고 설명합니다. 한가지는 256by256 이미지에서 수평대칭을 포함한 임의의 224by224 패치를 추출하는것 이라 하고, 다른 하나는 RGB 채널강도를 조절하는 것인데, 주어진 이미지 데이터에대해 PCA를 수행하여 eigenpair를 얻고, eigen value를 평균이 0이고 표준편차가 0.1인 가우시안 난수를 곱하여 변형하고 이를 주성분이 RGB 각 차원의 길이만큼에 곱하여 원래 RGB에 더하여 사용하였다고 설명합니다. 두번째 변환은 이것으로 주어진 데이터의 RGB 분포를 일반화해 주는 효과를 갖게될것으로 보입니다. 

2.  Dropout

    드롭아웃은 다양한 모델의 ensemble하여 사용하기 어려운 neuralnet의 좋은 보완책이 됩니다. dropout을 0.5로 설정하는것으로 훈련단계에서의 비용은 2배가 늘어나지만, 뉴런의 등장을 결정할때마다 다른 아키택처를 샘플링하는 효과를 가져옵니다. 이 모델에서는 앞의 두개의 fully connected layer에서 dropout을 적용하였다고 설명합니다. 


<a id="orge6c2f59"></a>

### Details of learning

SGD optimizer와 128 batch size를 이용하였다고 합니다. 이때, small weight decay를 사용하는것이 학습에 크게 도움이 되었다고 설명합니다. train dataset을 총 90cycles을 돌며 학습하였다고 하고 GTX580 두장을 이용하여 학습하는데에 총 5~6일이 소요되었다고 설명합니다.


<a id="org9601d98"></a>

### Results

AlexNet의 ILSVRC-2010, 2012에서 성능이 다른 과거의 모델에 비해 월등히 높다는 점을 강조합니다. 


<a id="orgc0519de"></a>

### Discussion

이 실험으로 기록적인 성과를 달성할 수 있었음을 강조하고 있습니다. convolutional architecture중 하나의 layer만 없애도 성능이 크게 떨어지기 때문에 neural network의 깊이에 의미가 있음을 한번 더 확인 합니다. supervised learning 이 아닌 unsupervised pre-training 이 도움이 되지 않을까 제안하고 있고, 또 더 크고 깊은 CNN을 설계하는것이 의미가 있을것임을 주장합니다.


<a id="orgf0c91c2"></a>

## Reference

-   Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25.
