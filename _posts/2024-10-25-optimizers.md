---
title: "Optimizer 개관"
date: 2024-10-25
layout: post
categories: 
- AI-Research
tags: 
- optimizer
---

Deep Learning 모델의 학습 과정에서 Optimizer는 매우 중요한 역할을 합니다. 개인적으로는 큰 고민없이 Adam과 AdamW를 사용해 왔으며, 그 의미를 대략적으로만 이해한채 활용해왔습니다. 이는 사용하면서 어딘가 찜찜한 기분을 남겼고, 이런이유로 한번은 명확하게 정리하면 좋겠다 싶었습니다. Optimizer와 관련하여 보기좋게 도식화한 그림이 있어서 가져왔습니다. 이 도식의 스토리라인에 맞춰 대표적인 것들에 대해서 포스팅을 해보려 합니다.


<a id="org25a86ad"></a>

## Optimizers


<a id="org7d70e9a"></a>

### SGD


<a id="org714265b"></a>

### Momentum


<a id="org7629606"></a>

### RMSProp


<a id="orga626a5e"></a>

### Adam


<a id="orgd0b28ad"></a>

### AdamW
