#+TITLE: (Transformer) Attention Is All You Need 리뷰
#+LAYOUT: post
#+jekyll_tags: nlp
#+jekyll_categories: AI-Research
#+DATE: 2024-10-31

- 상위포스트 : nlp분야 논문리뷰 및 구현
- 논문 : https://arxiv.org/pdf/1706.03762
- 구현 : https://github.com/000namc/paper-implementations/tree/main/vision

** (Transformer) Attention Is All You Need

https://000namc.xyz/nginx/blog/alexnet/figure1.jpeg

*** Abstract
 Transformer는 attention mechanism만을 이용한 simple한 모델이라고 설명합니다. 이는 이떄까지의 주된 sequencial 모델들이 CNN 혹은 RNN에 기반을 둔것 그리고 encoder와 decoder를 attention mechanism으로 연결한 과거 연구와 다릅니다. 두가지 기계 번역 테스크에서 이 모델의 성능이 우수하며 병렬화가 가능하고 훈련에 필요한 시간이 적다는것을 확인하였다고 합니다. WMT-2014 영어-프랑스어 번역작업에서 state of the art인 성능을 보였으며, 영어 구문 분석 작업에도 성공적으로 적용되었다고 합니다.

*** Introduction 
RNN, LSTM, GRN등이 언어 모델링과 기계 번역에서 활발히 연구되어 왔습니다. 이러한 RNN 모델들은 입력 및 출력 sequence의 위치에 따라 계산되는 식이 달라지는데, 예를들어 $h_t$ 를 계산할때 $h_{t-1}$ 에 대한 함수로써 계산하게 됩니다. 이러한 설계가 training 과정에서의 병렬화를 방해하고, 긴 sequnce의 입력 데이터에서 메모리 제약으로 인한 문제를 야기한다고 합니다. 

이 논문에서는 attention mechanism과 RNN의 구조를 함께 활용했던 연구들과 달리, 입력과 출력간의 전역적 의존성을 모델링하기 위해 attention mechanism만을 활용하는 Transformer를 제안한다고 합니다. 
*** Model Architecture
 encoder는 input sequence $x = (x_1, \cdots, x_n)$ 을 입력으로 하여 continuous representations $(z = (z_1, \cdots, z_n))$ 을 추출해 내는 구조를 갖고, decoder는 $z$ 를 입력으로 하여 최종적으로 fully connected layer를 거쳐 output sequence $y = (y_1,\cdots, y_m)$ 를 추출해내는 구조를 갖는다고 합니다.

https://000namc.xyz/nginx/blog/transformer/figure2.jpeg

**** Encoder and Decoder Stacks
- Encoder는 6개의 encoder layer를 쌓아 구성합니다. 각 encoder layer는 두개의 sub layer, multi-head self-attention layer와 linear layer로 구성되며 각각 layer를 normalization을 하는데에 residual connection을 적용합니다.
- Decoder도 6개의 decoder layer를 쌓아 구성합니다. encoder layer와 비슷하게 구성하되 세개의 sub layer로 이루어지는데, 2개의multi-head self-attention layer와 linear layer로 구성되며 중간의 self-attention layer는 첫번째 self-attention의 결과 sequence와 Encoder의 결과 sequence를 입력으로 하는 구조를 갖는다고 합니다.
 
  #+BEGIN_SRC
    Encoder = concat(
        encoder_layer,
        encoder_layer,
        encoder_layer,
        encoder_layer,
        encoder_layer,
        encoder_layer
    )

    encoder_layer =  (
        .multi-head_self-attention()
        .norm_with_residual()
        .linear()
        .norm_with_residual()
    )
  #+END_SRC

  #+BEGIN_SRC
    Decoder = concat(
        decoder_layer,
        decoder_layer,
        decoder_layer,
        decoder_layer,
        decoder_layer,
        decoder_layer
    )

    decoder_layer =  (
        .multi-head_self-attention()
        .norm_with_residual()
        .multi-head_self-attention(z, )
        .norm_with_residual()
        .linear()
        .norm_with_residual()
    )
  #+END_SRC
**** Attention
adf
**** Position-wise Feed Forward Networks

**** Embeddings and Softmax 

**** Positional Encoding

*** Why Self-Attention
*** Training

**** Training Data and Batching

**** Hardware and Schedule

**** Optimizer

**** Regularization

*** Results

**** Machine Translation

**** Model Variations 

**** English Constituency Parsing

*** Conclusion
