#+TITLE: VGG 논문 리뷰
#+LAYOUT: post
#+jekyll_tags: paper-review vision
#+jekyll_categories: AI-Research
#+DATE: 2024-10-21


- 상위포스트 : vision 분야 논문리뷰 및 구현
- 논문 : https://arxiv.org/pdf/1409.1556
- 구현 : https://github.com/000namc/paper-implementations/tree/main/vision


** 논문 초록
** (VGG) Very Deep Convolutional Networks For Large-Scale Image Recognition
 VGG는 이미지 분류 작업에서 convolution network의 깊이가 정확도에 크게 영향을 줌을 밝힌 연구입니다. 이 모델은 매우 작은 3by3 convolution filter를 사용하는것이 특징이며, network의 깊이를 16~19 까지 확장하였다고 합니다. 이러한 설계로 2014년 imagenet challenge 및 다른 밴치마크 데이터셋에서 state of art 인 성능을 보였다고 설명합니다.
*** Introduction
 Convolutional Network는 최근 대규모 이미지 및 비디오 인식분야에서 큰 성공을 거두고 있다고 설명합니다. 그 아키텍쳐에 관해 smaller stride 그리고 smaller window size를 선택하는 등의 개선작업이 이어지고 있는데, 이 논문에서는 또 하나의 중요한 측면인 깊이를 다루겠다고 설명합니다. 다른 매개변수는 고정하고 네트워크의 깊이를 늘린 결과 ILSVRC-2014에서 State of the art 를 달성했다고 합니다.  
*** Convnet Configurations
**** Architecture
 입력 데이터는 224by224인 이미지 라고 합니다. 이 데이터에 적용하는 유일한 전처리는 학습데이터의 각 픽셀의 평균 RGB값을 빼는 처리라고 설명합니다. 16~19 깊이까지의 convolution layer에 사용되는 kernel은 매우 작은 3by3 filter를 사용한다고 합니다. 가장 마지막에는 세개의 fully connected layer로 구성이 되고, 앞의 두 layer는 4096개의 channel을, 마지막 layer에는 1000개의 channel로 구성하여 1000개의 클래스에 대한 ILSVRC 분류를 한다고 설명합니다. activation function으로 ReLU를 활용하고, LRN 정규화는 크게 성능을 높이는데 기여하지않아 사용하지 않는다고 합니다. 
**** Configurations
 convolution network의 구성은 아래 표와같이 구성하였다고 합니다. architecture는 가벼운 모델에서부터 무거운 모델까지 A~E라 이름을 붙이고 있습니다. 가장 무거운 구성을 보면, 19layers로 구성되며 64channel 에서부터 max pooling layer를 거치며 2배씩 그 channel을 늘려 나갑니다. 이렇게 구성하였을때, kernel을 작게 고정한 이점으로 깊이는 얇고 더 큰 kernel을 사용한 모델에 비해 parameter수가 결코 많지 않음을 강조하고 있습니다.  
**** Discussion
 kernel size를 더 크게 잡는것과 이 연구에서 설정한 3by3 으로 잡는것을 비교합니다.
*** Classfication Framework
**** Training
SGD optimizer와 256 batch size를 이용하였다고 합니다. 이때 optimizer의 momentum은 0.9, weight decay는 0.0005로 설정하였다고 합니다. fully connected layer의 앞 두개의 layer에 0.5의 dropout을 적용하였다고 합니다. learning rate는 0.01로 설정한 후 검증 데이터셋에서의 성능향상이 이루어지지 않을때마다 10배씩 줄여 사용하였다고 설명합니다. 네트워크의 가중치 초기화를 어떻게 하느냐가 학습에 큰 영향을 준다고 주장합니다. 가장 작은 스케일의 A architecture를 랜덤 초기화하여 학습시킨후 점진적으로 큰 모델의 초기값을 이전 스케일의 학습된 가중치를 일부 활용함으로써 학습에 도움이 되었다고 설명합니다. 
**** Testing
...
**** Implementation Details
학습에 C++의 Caffe 툴박스를 이용하였으며, NVIDIA Titan Black GPU 4장을 사용하여 2~3주 가 소요되었다고 설명합니다.
*** Classification Experiments
**** Single Scale Evaluation
**** Multi Scale Evaluation
**** Multi Crop Evaluation
**** Convnet Fusion
**** Comparison With The State Of The Art
*** Conclusion
