#+TITLE: Optimizer 개관
#+LAYOUT: post
#+jekyll_tags: optimizer
#+jekyll_categories: AI-Research
#+DATE: 2024-10-25

Deep Learning 모델의 학습 과정에서 Optimizer는 매우 중요한 역할을 합니다. 개인적으로는 큰 고민없이 Adam과 AdamW를 사용해 왔으며, 그 의미를 대략적으로만 이해한채 활용해왔습니다. 이는 사용하면서 어딘가 찜찜한 기분을 남겼고, 이런이유로 한번은 명확하게 정리하면 좋겠다 싶었습니다. Optimizer와 관련하여 보기좋게 도식화한 그림이 있어서 가져왔습니다. 이 도식의 스토리라인에 맞춰 대표적인 것들에 대해서 포스팅을 해보려 합니다.

** Optimizers
*** SGD
*** Momentum
*** RMSProp
*** Adam
*** AdamW


