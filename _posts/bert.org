#+TITLE: (BERT) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 리뷰
#+LAYOUT: post
#+jekyll_tags: nlp
#+jekyll_categories: AI-Research
#+DATE: 2024-11-05

- 상위포스트 : nlp 분야 논문리뷰 및 구현
- 논문 : https://arxiv.org/pdf/1810.04805
- 구현 : https://github.com/000namc/paper-implementations/tree/main/vision

<hr>

https://000namc.xyz/nginx/blog/bert/figure1.jpeg
 
*** Abstract
 BERT는 모든 층에서 좌우 문맥을 동시에 고려한 양방향 표현을 사전학습하도록 설계하였다고 합니다. 이 사전 학습된 BERT 모델은 추가적인 출력층 만으로도 다양한 작업을 위한 모델로 fine-tuning 이 가능하게 되는데, 이는 개념적으로 간단하면서도 강력하다고 설명합니다. 

*** Introduction
이 논문이 출판되던 2018년 즈음에는 차차 언어 모델 사전학습이 많은 자연어 처리 문제들에서 효과적임이 입증되고 있었습니다. 언어 모델을 학습하는 데에는 두가지 방법론이 있는데, feature-based 와 fine-tuning으로 나뉘어 집니다. feature-based 방법은 ELMo model처럼 언어 모델의 representation을 additional 한 feature로써 활용하고, fine-tuning 방법은 GPT model처럼 additional한 학습가능한 parameter를 추가하고 해당 parameter들을 특정 테스크에 fine tuning을 하는것입니다. 두 방법 모두 사전학습된 단방향의 언어모델을 사용하는점에서 동일하겠습니다.

언어 모델이 단방향으로만 작동하는 것은 특정 테스크에서는 유리할 수 있으나 일반적인 자연어 처리 문제들을 다루기에는 무리가 있다고 서술하고 있습니다. 그런의미에서  이 연구에서는 양방향의 맥락을 고려할 수 있는 BERT를 제안합니다. 양방향의 사전학습이 가능 할 수 있도록 MLM task를 제안합니다. 최종적으로는 MLM과 NSP를 이용하여 사전학습을 구성하였다고 합니다. 

이러한 구성으로 11개의 nlp task에서 state-of-the-art인 결과를 얻어 낼 수 있었다고 합니다. 
*** Related Work
**** Unsupervised Feature-based Approaches
 단어의 백터 표현에 대해서는 수십년간 연구가 계속되어 왔다고 합니다. 비신경망 모델에서 부터 신경망 모델에 까지, 문장 임베딩이나 단락임베딩 과 같이 다양한 방향으로 세분화 되어 연구되어 왔고, 이러한 연구가 지속되던 중 LSTM을 근간으로 맥락을 이해할 수 있도록 설계된 ELMo모델이 등장 했다고 합니다. 이는 입력된 문장의 단방향의 맥락을 파악 할 수 있는 형태로 구성되었고, 질문 응답, 감정분석, 개체인식 등의 문제에서 좋은 성능을 기록 할 수 있었다합니다.   
**** Unsupervised Fine-tuning Approacahes
 사전학습된 모델을 이용하여 downstream task에 fine-tunging하는 연구들이 이루어져 왔습니다. 이는 조금의 parameter만 학습해도 된다는 장점을 가지고 있고, 이러한 방향의 연구의 예시로써 OPENAI의 GPT는 GLUE 벤치마크 데이터셋에서 state-of-the-art인 결과를 얻어낼 수 있었다고 합니다. 
**** Transfer Learning from Supervised Data
supervised 세팅에서 사전학습을 진행한 후 fine-tuning을 했을때도 효과적이라는 연구도 이루어져 왔습니다. 
*** BERT
**** Architecture
https://000namc.xyz/nginx/blog/bert/figure2.jpeg
BERT는 양방향의 Transformer encoder를 기반으로 한 모델입니다. encoder block 수를 L, 히든 사이즈를 H, 셀프 어탠션 헤드를 A라고 할때,
BERTbase  (L=12, H=768, A=12), BERTlarge  (L=24, H=1024, A=16) 을 각각 정의합니다. BERTbase는 GPT와 비교하기위해 똑같은 크기를 갖도록 선택하였는데, BERT가 양방향 셀프 어텐션을 사용한다는 점에서 그 차별점이 있습니다.  

https://000namc.xyz/nginx/blog/bert/figure3.jpeg
BERT는 더많은 문제에 활용가능하도록 구성하기 위해 single sentence와 pair of sentence를 모두 입력으로 사용할 수 있도록 만들었습니다. WordPiece tokenizer를 활용하여 30000개의 단어 사전(각각은 토큰 이라 부름)을 구성하였고, 맨 앞의 토큰은 [CLS] sentence 마지막의 토큰은 [SEP] 으로 구성하였습니다. CLS토큰은 classification을 위해 최종적으로 활용하기 위해 구성한 토큰으로, 문장의 핵심이 되는 정보가 담기도록 구성합니다.  BERT의 최종적인 input representation은 각 단어 토큰 embedding과 sentence를 구분하기위한 segment embedding, trasformer모델과 마찬가지의 position embedding의 합으로 구성합니다. 

**** Pre-training
BERT를 사전학습하는데에 두가지 방법이 있습니다. 하나는 MLM, 그리고 NSP입니다.
- Task 1 : (MLM) Masked LM

- Task 2 : (NSP) Next Sentence Prediction
  
**** Fine-tuning
*** Experiments

*** Ablation Studies
to be written
*** Conclusion
언어 모델의 사전학습이 많은 자연어 처리 문제에 도움이 됨을 강조 하고 있습니다. 또, 제안된 모델 BERT의 경우 그중에서도 양방향 아키텍처로 일반화한 부분에서 그 차별점이 있음을 밝히고 있습니다. 
*** Reference
Kenton, Jacob Devlin Ming-Wei Chang, and Lee Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." Proceedings of naacL-HLT. Vol. 1. 2019.
