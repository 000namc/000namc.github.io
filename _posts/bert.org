#+TITLE: (BERT) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 리뷰
#+LAYOUT: post
#+jekyll_tags: nlp
#+jekyll_categories: AI-Research
#+DATE: 2024-11-05

- 상위포스트 : nlp 분야 논문리뷰 및 구현
- 논문 : https://arxiv.org/pdf/1810.04805
- 구현 : https://github.com/000namc/paper-implementations/tree/main/vision

<hr>

https://000namc.xyz/nginx/blog/bert/figure1.jpeg
 
*** Abstract
 BERT는 모든 층에서 좌우 문맥을 동시에 고려한 양방향 표현을 사전학습하도록 설계하였다고 합니다. 이 사전 학습된 BERT 모델은 추가적인 출력층 만으로도 다양한 작업을 위한 모델로 fine-tuning 이 가능하게 되는데, 이는 개념적으로 간단하면서도 강력하다고 설명합니다. 

*** Introduction
이 논문이 출판되던 2018년 즈음에는 차차 언어 모델 사전학습이 많은 자연어 처리 문제들에서 효과적임이 입증되고 있었습니다. 언어 모델을 학습하는 데에는 두가지 방법론이 있는데, feature-based 와 fine-tuning으로 나뉘어 집니다. feature-based 방법은 ELMo model처럼 언어 모델의 representation을 additional 한 feature로써 활용하고, fine-tuning 방법은 GPT model처럼 additional한 학습가능한 parameter를 추가하고 해당 parameter들을 특정 테스크에 fine tuning을 하는것입니다. 두 방법 모두 사전학습된 단방향의 언어모델을 사용하는점에서 동일하겠습니다.

언어 모델이 단방향으로만 작동하는 것은 특정 테스크에서는 유리할 수 있으나 일반적인 자연어 처리 문제들을 다루기에는 무리가 있다고 서술하고 있습니다. 그런의미에서  이 연구에서는 양방향의 맥락을 고려할 수 있는 BERT를 제안합니다. 양방향의 사전학습이 가능 할 수 있도록 MLM task를 제안합니다. 최종적으로는 MLM과 NSP를 이용하여 사전학습을 구성하였다고 합니다. 

이러한 구성으로 11개의 nlp task에서 state-of-the-art인 결과를 얻어 낼 수 있었다고 합니다. 
*** Related Work
**** Unsupervised Feature-based Approaches
 단어의 백터 표현에 대해서는 수십년간 연구가 계속되어 왔다고 합니다. 비신경망 모델에서 부터 신경망 모델에 까지, 문장 임베딩이나 단락임베딩 과 같이 다양한 방향으로 세분화 되어 연구되어 왔고, 이러한 연구가 지속되던 중 LSTM을 근간으로 맥락을 이해할 수 있도록 설계된 ELMo모델이 등장 했다고 합니다. 이는 입력된 문장의 단방향의 맥락을 파악 할 수 있는 형태로 구성되었고, 질문 응답, 감정분석, 개체인식 등의 문제에서 좋은 성능을 기록 할 수 있었다합니다.   
**** Unsupervised Fine-tuning Approacahes
 사전학습된 모델을 이용하여 downstream task에 fine-tunging하는 연구들이 이루어져 왔습니다. 이는 조금의 parameter만 학습해도 된다는 장점을 가지고 있고, 이러한 방향의 연구의 예시로써 OPENAI의 GPT는 GLUE 벤치마크 데이터셋에서 state-of-the-art인 결과를 얻어낼 수 있었다고 합니다. 
**** Transfer Learning from Supervised Data
supervised 세팅에서 사전학습을 진행한 후 fine-tuning을 했을때도 효과적이라는 연구도 이루어져 왔습니다. 
*** BERT
**** Architecture
https://000namc.xyz/nginx/blog/bert/figure2.jpeg
BERT는 양방향의 Transformer encoder를 기반으로 한 모델입니다. encoder block 수를 L, 히든 사이즈를 H, 셀프 어탠션 헤드를 A라고 할때,
BERTbase  (L=12, H=768, A=12), BERTlarge  (L=24, H=1024, A=16) 을 각각 정의합니다. BERTbase는 GPT와 비교하기위해 똑같은 크기를 갖도록 선택하였는데, BERT가 양방향 셀프 어텐션을 사용한다는 점에서 그 차별점이 있습니다.  

https://000namc.xyz/nginx/blog/bert/figure3.jpeg
BERT는 더많은 문제에 활용가능하도록 구성하기 위해 single sentence와 pair of sentence를 모두 입력으로 사용할 수 있도록 만들었습니다. WordPiece tokenizer를 활용하여 30000개의 단어 사전(각각은 토큰 이라 부름)을 구성하였고, 맨 앞의 토큰은 [CLS] sentence 마지막의 토큰은 [SEP] 으로 구성하였습니다. CLS토큰은 classification을 위해 최종적으로 활용하기 위해 구성한 토큰으로, 문장의 핵심이 되는 정보가 담기도록 구성합니다.  BERT의 최종적인 input representation은 각 단어 토큰 embedding과 sentence를 구분하기위한 segment embedding, trasformer모델과 마찬가지의 position embedding의 합으로 구성합니다. 

**** Pre-training
BERT를 사전학습하는데에 두가지 방법이 있습니다.
- Task 1 : (MLM) Masked LM
깊은 양방향 표현을 학습시키기 위해, 입력 토큰의 일부를 무작위로 마스킹하고, 이러한 마스킹된 토큰을 예측하는 테스크를 구성할 필요가 있겠습니다. 이러한 구조를 MLM이라 부릅니다. 이 연구에서는 모든 실험에서 WordPiece 토큰 중 15%를 무작위로 마스킹 하였다고 합니다. 단, 이러한 방법에서의 사전학습은 fine-tuning을 하는 상황과 괴리가 있겠습니다. fine-tuning 단계에서 [MASK] token이 나타나지 않기 때문인데, 이런이유로 선택된 위치에 대해 입력토큰의 80% 만 [MASK] 토큰으로 대체하고 10% 확률로 임의 토큰, 10% 확률로 원래 토큰을 그대로 유지한다고 합니다.
- Task 2 : (NSP) Next Sentence Prediction
문장간의 관계를 학습시키기 위해, 언어 코퍼스에서 이어지는 문장을 이용해 다음 문장을 예측하게 하는 테스크를 구성할 필요가 있겠습니다. 이러한 구조를 NSP라 부릅니다. 구체적으로, 문장 A, B를 선택할때 50%는 문장 B를 실제로 A 다음에 이어지는 문장으로 설정하고, 나머지 50%는 무작위 문장으로 설정 하였다고 합니다. 이러한 구성의 사전학습이 QA와 NLI 테스크에서 유효함을 확인하였다고 합니다.
**** Pre-training data
사전 훈련 코퍼스로는 BooksCorpus (8억 단어)와 English Wikipedia (25억 단어)를 사용하였다 합니다. 
**** Fine-tuning
Transformer의 Self Attention 메커니즘 덕분에 단일 sentence나 sentence pair를 포함하는 다양한 downstream task를 처리할 수 있겠습니다. 각 작업별 입력과 출력을 BERT에 연결하고 모든 parameter를 end-to-end로 fine-tune 할 수 있습니다. 출력에서의 token embedding은, 시퀀스 태깅이나 질의응답 같은 token level의 task에서 사용되고, [CLS]의 embedding은 entailment나 감성분류 등의 일반적인 분류 작업에서 활용합니다. 
*** Experiments
https://000namc.xyz/nginx/blog/bert/figure4.jpeg
다양한 benchmark dataset에서 성능을 기록하고 있습니다. 생략합니다. 
*** Ablation Studies
to be written
*** Conclusion
언어 모델의 사전학습이 많은 자연어 처리 문제에 도움이 됨을 강조 하고 있습니다. 또, 제안된 모델 BERT의 경우 그중에서도 양방향 아키텍처로 일반화한 부분에서 그 차별점이 있음을 밝히고 있습니다. 
*** Reference
Kenton, Jacob Devlin Ming-Wei Chang, and Lee Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." Proceedings of naacL-HLT. Vol. 1. 2019.
